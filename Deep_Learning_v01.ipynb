{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load Deep_Learning_v01.py\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import datetime\n",
    "import time\n",
    "import sys\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import platform\n",
    "import time as tm\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.cross_validation import KFold, StratifiedShuffleSplit\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input,BatchNormalization,merge\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D,AveragePooling2D\n",
    "from keras.optimizers import Adam, Adagrad, SGD\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications import vgg16\n",
    "\n",
    "import tensorflow as tf1\n",
    "\n",
    "np.random.seed(2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Deep learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d_bn(x, nb_filter, nb_row, nb_col,\n",
    "              border_mode='same', subsample=(1, 1),\n",
    "              name=None):\n",
    "    \"\"\"Utility function to apply conv + BN.\n",
    "    \"\"\"\n",
    "    if name is not None:\n",
    "        bn_name = name + '_bn'\n",
    "        conv_name = name + '_conv'\n",
    "    else:\n",
    "        bn_name = None\n",
    "        conv_name = None\n",
    "\n",
    "    x = Convolution2D(nb_filter, nb_row, nb_col,\n",
    "                      subsample=subsample,\n",
    "                      activation='relu',\n",
    "                      border_mode=border_mode,\n",
    "                      name=conv_name)(x)\n",
    "    x = BatchNormalization(axis=channel_axis, name=bn_name)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inception Network\n",
    "\n",
    "<img src=\"https://4.bp.blogspot.com/-TMOLlkJBxms/Vt3HQXpE2cI/AAAAAAAAA8E/7X7XRFOY6Xo/s1600/image03.png\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CNN_Inceptionv03_Classifier(img_rows, img_cols, color_type,num_category):\n",
    "    global predict_proba\n",
    "    predict_proba  = False\n",
    "\n",
    "    # Determine proper input shape\n",
    "    input_shape1 = _obtain_input_shape(input_shape,\n",
    "                                      default_size=299,\n",
    "                                      min_size=139,\n",
    "                                      dim_ordering=K.image_dim_ordering(),\n",
    "                                      include_top=None)\n",
    "\n",
    "    img_input = Input(shape=input_shape1)\n",
    "\n",
    "    x = conv2d_bn(img_input, 32, 3, 3, subsample=(2, 2), border_mode='valid')\n",
    "    x = conv2d_bn(x, 32, 3, 3, border_mode='valid')\n",
    "    x = conv2d_bn(x, 64, 3, 3)\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "    x = conv2d_bn(x, 80, 1, 1, border_mode='valid')\n",
    "    x = conv2d_bn(x, 192, 3, 3, border_mode='valid')\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "    # mixed 0, 1, 2: 35 x 35 x 256\n",
    "    for i in range(3):\n",
    "        branch1x1 = conv2d_bn(x, 64, 1, 1)\n",
    "\n",
    "        branch5x5 = conv2d_bn(x, 48, 1, 1)\n",
    "        branch5x5 = conv2d_bn(branch5x5, 64, 5, 5)\n",
    "\n",
    "        branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n",
    "        branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "        branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "\n",
    "        branch_pool = AveragePooling2D(\n",
    "            (3, 3), strides=(1, 1), border_mode='same')(x)\n",
    "        branch_pool = conv2d_bn(branch_pool, 32, 1, 1)\n",
    "        x = merge([branch1x1, branch5x5, branch3x3dbl, branch_pool],\n",
    "                  mode='concat', concat_axis=channel_axis,\n",
    "                  name='mixed' + str(i))\n",
    "\n",
    "    # mixed 3: 17 x 17 x 768\n",
    "    branch3x3 = conv2d_bn(x, 384, 3, 3, subsample=(2, 2), border_mode='valid')\n",
    "\n",
    "    branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3,\n",
    "                             subsample=(2, 2), border_mode='valid')\n",
    "\n",
    "    branch_pool = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "    x = merge([branch3x3, branch3x3dbl, branch_pool],\n",
    "              mode='concat', concat_axis=channel_axis,\n",
    "              name='mixed3')\n",
    "\n",
    "    # mixed 4: 17 x 17 x 768\n",
    "    branch1x1 = conv2d_bn(x, 192, 1, 1)\n",
    "\n",
    "    branch7x7 = conv2d_bn(x, 128, 1, 1)\n",
    "    branch7x7 = conv2d_bn(branch7x7, 128, 1, 7)\n",
    "    branch7x7 = conv2d_bn(branch7x7, 192, 7, 1)\n",
    "\n",
    "    branch7x7dbl = conv2d_bn(x, 128, 1, 1)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 7, 1)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 1, 7)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 7, 1)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n",
    "\n",
    "    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), border_mode='same')(x)\n",
    "    branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n",
    "    x = merge([branch1x1, branch7x7, branch7x7dbl, branch_pool],\n",
    "              mode='concat', concat_axis=channel_axis,\n",
    "              name='mixed4')\n",
    "\n",
    "    # mixed 5, 6: 17 x 17 x 768\n",
    "    for i in range(2):\n",
    "        branch1x1 = conv2d_bn(x, 192, 1, 1)\n",
    "\n",
    "        branch7x7 = conv2d_bn(x, 160, 1, 1)\n",
    "        branch7x7 = conv2d_bn(branch7x7, 160, 1, 7)\n",
    "        branch7x7 = conv2d_bn(branch7x7, 192, 7, 1)\n",
    "\n",
    "        branch7x7dbl = conv2d_bn(x, 160, 1, 1)\n",
    "        branch7x7dbl = conv2d_bn(branch7x7dbl, 160, 7, 1)\n",
    "        branch7x7dbl = conv2d_bn(branch7x7dbl, 160, 1, 7)\n",
    "        branch7x7dbl = conv2d_bn(branch7x7dbl, 160, 7, 1)\n",
    "        branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n",
    "\n",
    "        branch_pool = AveragePooling2D(\n",
    "            (3, 3), strides=(1, 1), border_mode='same')(x)\n",
    "        branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n",
    "        x = merge([branch1x1, branch7x7, branch7x7dbl, branch_pool],\n",
    "                  mode='concat', concat_axis=channel_axis,\n",
    "                  name='mixed' + str(5 + i))\n",
    "\n",
    "    # mixed 7: 17 x 17 x 768\n",
    "    branch1x1 = conv2d_bn(x, 192, 1, 1)\n",
    "\n",
    "    branch7x7 = conv2d_bn(x, 192, 1, 1)\n",
    "    branch7x7 = conv2d_bn(branch7x7, 192, 1, 7)\n",
    "    branch7x7 = conv2d_bn(branch7x7, 192, 7, 1)\n",
    "\n",
    "    branch7x7dbl = conv2d_bn(x, 160, 1, 1)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 7, 1)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 7, 1)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n",
    "\n",
    "    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), border_mode='same')(x)\n",
    "    branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n",
    "    x = merge([branch1x1, branch7x7, branch7x7dbl, branch_pool],\n",
    "              mode='concat', concat_axis=channel_axis,\n",
    "              name='mixed7')\n",
    "\n",
    "    # mixed 8: 8 x 8 x 1280\n",
    "    branch3x3 = conv2d_bn(x, 192, 1, 1)\n",
    "    branch3x3 = conv2d_bn(branch3x3, 320, 3, 3,\n",
    "                          subsample=(2, 2), border_mode='valid')\n",
    "\n",
    "    branch7x7x3 = conv2d_bn(x, 192, 1, 1)\n",
    "    branch7x7x3 = conv2d_bn(branch7x7x3, 192, 1, 7)\n",
    "    branch7x7x3 = conv2d_bn(branch7x7x3, 192, 7, 1)\n",
    "    branch7x7x3 = conv2d_bn(branch7x7x3, 192, 3, 3,\n",
    "                            subsample=(2, 2), border_mode='valid')\n",
    "\n",
    "    branch_pool = AveragePooling2D((3, 3), strides=(2, 2))(x)\n",
    "    x = merge([branch3x3, branch7x7x3, branch_pool],\n",
    "              mode='concat', concat_axis=channel_axis,\n",
    "              name='mixed8')\n",
    "\n",
    "    # mixed 9: 8 x 8 x 2048\n",
    "    for i in range(2):\n",
    "        branch1x1 = conv2d_bn(x, 320, 1, 1)\n",
    "\n",
    "        branch3x3 = conv2d_bn(x, 384, 1, 1)\n",
    "        branch3x3_1 = conv2d_bn(branch3x3, 384, 1, 3)\n",
    "        branch3x3_2 = conv2d_bn(branch3x3, 384, 3, 1)\n",
    "        branch3x3 = merge([branch3x3_1, branch3x3_2],\n",
    "                          mode='concat', concat_axis=channel_axis,\n",
    "                          name='mixed9_' + str(i))\n",
    "\n",
    "        branch3x3dbl = conv2d_bn(x, 448, 1, 1)\n",
    "        branch3x3dbl = conv2d_bn(branch3x3dbl, 384, 3, 3)\n",
    "        branch3x3dbl_1 = conv2d_bn(branch3x3dbl, 384, 1, 3)\n",
    "        branch3x3dbl_2 = conv2d_bn(branch3x3dbl, 384, 3, 1)\n",
    "        branch3x3dbl = merge([branch3x3dbl_1, branch3x3dbl_2],\n",
    "                             mode='concat', concat_axis=channel_axis)\n",
    "\n",
    "        branch_pool = AveragePooling2D(\n",
    "            (3, 3), strides=(1, 1), border_mode='same')(x)\n",
    "        branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n",
    "        x = merge([branch1x1, branch3x3, branch3x3dbl, branch_pool],\n",
    "                  mode='concat', concat_axis=channel_axis,\n",
    "                  name='mixed' + str(9 + i))\n",
    "\n",
    "    # Classification block\n",
    "    x = AveragePooling2D((8, 8), strides=(8, 8), name='avg_pool')(x)\n",
    "    x = Flatten(name='flatten')(x)\n",
    "    #x = Dense(1000, activation='softmax', name='predictions1')(x)\n",
    "    x = Dense(num_category, activation='softmax', name='predictions')(x)\n",
    "\n",
    "    model = Model(input =img_input, output=x, name='inception_v3')\n",
    "    sgd = SGD(lr=1e-2, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(optimizer=sgd, loss='categorical_crossentropy')\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inception Network in-built in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CNN_Inceptionv03_inbuilt_Classifier(img_rows, img_cols, color_type,num_category):\n",
    "    global predict_proba\n",
    "    predict_proba  = False\n",
    "\n",
    "    print('Loading InceptionV3 Weights ...')\n",
    "    InceptionV3_notop = InceptionV3(include_top=False, weights='imagenet',input_tensor=None,input_shape=input_shape)\n",
    "\n",
    "    print('Adding Average Pooling Layer and Softmax Output Layer ...')\n",
    "    output = InceptionV3_notop.get_layer(index = -1).output\n",
    "    output = AveragePooling2D((8, 8), strides=(8, 8), name='avg_pool')(output)\n",
    "    output = Flatten(name='flatten')(output)\n",
    "    output = Dense(num_category, activation='softmax', name='predictions')(output)\n",
    "    model = Model(InceptionV3_notop.input, output)\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    optimizer = SGD(lr = 1e-3, momentum = 0.9, decay = 0.0, nesterov = True)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer = optimizer, metrics = ['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up VGG16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def CNN_VGG16_Classifier(img_rows, img_cols, color_type,num_category):\n",
    "    global predict_proba\n",
    "\n",
    "    predict_proba  = False\n",
    "    img_input = Input(shape=input_shape)\n",
    "\n",
    "    # Block 1\n",
    "    x = Convolution2D(64, 3, 3, activation='relu', border_mode='same', name='block1_conv1')(img_input)\n",
    "    x = Convolution2D(64, 3, 3, activation='relu', border_mode='same', name='block1_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "    #Block 2\n",
    "    x = Convolution2D(128, 3, 3, activation='relu', border_mode='same', name='block2_conv1')(x)\n",
    "    x = Convolution2D(128, 3, 3, activation='relu', border_mode='same', name='block2_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "    #Block 3\n",
    "    x = Convolution2D(256, 3, 3, activation='relu', border_mode='same', name='block3_conv1')(x)\n",
    "    x = Convolution2D(256, 3, 3, activation='relu', border_mode='same', name='block3_conv2')(x)\n",
    "    x = Convolution2D(256, 3, 3, activation='relu', border_mode='same', name='block3_conv3')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "    #Block 4\n",
    "    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block4_conv1')(x)\n",
    "    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block4_conv2')(x)\n",
    "    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block4_conv3')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "    #Block 5\n",
    "    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block5_conv1')(x)\n",
    "    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block5_conv2')(x)\n",
    "    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block5_conv3')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n",
    "\n",
    "    x = Flatten(name='flatten')(x)\n",
    "    x = Dense(4096, activation='relu', name='fc1')(x)\n",
    "    x = Dense(4096, activation='relu', name='fc2')(x)\n",
    "    x = Dense(num_category, activation='softmax', name='predictions')(x)\n",
    "\n",
    "    model = Model(input =img_input, output=x, name='vgg16')\n",
    "    #optimizer = SGD(lr=1e-2, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    #optimizer = Adam(lr=1e-3)\n",
    "    optimizer = SGD(lr=1e-2, decay=1e-4, momentum=0.89, nesterov=True)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up VGG16 , inbuilt in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CNN_VGG16_inbuilt_Classifier(img_rows, img_cols, color_type,num_category):\n",
    "    global predict_proba\n",
    "    predict_proba  = False\n",
    "\n",
    "    print('Loading VGG16 Weights ...')\n",
    "\n",
    "    VGG16_notop = vgg16.VGG16(include_top=False, weights=None,\n",
    "          input_tensor=None, input_shape=input_shape)\n",
    "\n",
    "    print('Adding Average Pooling Layer and Softmax Output Layer ...')\n",
    "    output = VGG16_notop.get_layer(index = -1).output\n",
    "\n",
    "    output = Flatten(name='flatten')(output)\n",
    "    output = Dense(96, activation='relu',init='he_uniform')(output)\n",
    "    output = Dropout(0.4)(output)\n",
    "    output = Dense(24, activation='relu',init='he_uniform')(output)\n",
    "    output = Dropout(0.2)(output)\n",
    "    output = Dense(num_category, activation='softmax')(output)\n",
    "    model = Model(VGG16_notop.input, output)\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    optimizer = SGD(lr=1e-2, decay=1e-4, momentum=0.89, nesterov=True)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy')\n",
    "\n",
    "    #print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up parms for a customized CNN model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CNN_Classifier1(img_rows, img_cols, color_type,num_category):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((1, 1), input_shape=input_shape))\n",
    "    model.add(Convolution2D(8, 3, 3, activation='relu', init='he_uniform'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(16, 3, 3, activation='relu', init='he_uniform'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(96, activation='relu',init='he_uniform'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(24, activation='relu',init='he_uniform'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(num_category, activation='softmax'))\n",
    "\n",
    "    optimizer = SGD(lr=1e-2, decay=1e-4, momentum=0.89, nesterov=True)\n",
    "    #optimizer = Adagrad(lr=1e-3, epsilon=1e-08)\n",
    "    #optimizer = Adam(lr=1e-3)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge predicted outputs from multiple folds (simple avg ensembling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Merge_CV_folds_mean(data, nfolds):\n",
    "\n",
    "    print(\"Merge predicted outputs....\")\n",
    "    a = np.array(data[0])\n",
    "    for i in range(1, nfolds):\n",
    "        a += np.array(data[i])\n",
    "    a /= nfolds\n",
    "    return a.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create final output file (after classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def Create_final_output_file(predictions, test_id):\n",
    "\n",
    "    print(\"Create final predicted dataset....\")\n",
    "    predictions = np.clip(predictions,0.02, 0.98, out=None)\n",
    "\n",
    "    temp_pred = pd.DataFrame(predictions)\n",
    "    cols = lbl_y.inverse_transform(temp_pred.columns)\n",
    "\n",
    "    pred_DF = pd.DataFrame(predictions,columns=cols)\n",
    "    pred_DF['Predicted'] = pred_DF.idxmax(axis=1).str.strip()\n",
    "    pred_DF.insert(0, 'image', test_id)\n",
    "    pred_DF['Actual'] = pred_DF['image'].str.split('_', 1).str[0].str.strip()\n",
    "\n",
    "    pred_DF['match'] = np.where(pred_DF['Predicted']==pred_DF['Actual'],1,0)\n",
    "\n",
    "    print(\"Accuracy pct is \" + str(len(pred_DF[pred_DF['match'] ==1])*100/float(len(pred_DF))))\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "\n",
    "    suffix = str(now.strftime(\"%Y-%m-%d-%H-%M\"))\n",
    "    sub_file = os.path.join(file_path_orig,'results', 'result_' + suffix + '.csv')\n",
    "    pred_DF.to_csv(sub_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data_augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_augmentation(input_data,input_target,type):\n",
    "\n",
    "    if type == \"train\":\n",
    "        # this is the augmentation configuration we will use for training\n",
    "        aug_datagen = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            shear_range=0.1,\n",
    "            zoom_range=0.1,\n",
    "            rotation_range=10.,\n",
    "            width_shift_range=0.1,\n",
    "            height_shift_range=0.1,\n",
    "            horizontal_flip=True)\n",
    "\n",
    "        # aug_datagen = ImageDataGenerator(\n",
    "        #     rotation_range=40,\n",
    "        #     width_shift_range=0.2,\n",
    "        #     height_shift_range=0.2,\n",
    "        #     shear_range=0.2,\n",
    "        #     zoom_range=0.2,\n",
    "        #     horizontal_flip=True,\n",
    "        #     fill_mode='nearest')\n",
    "\n",
    "        # compute quantities required for featurewise normalization\n",
    "        # (std, mean, and principal components if ZCA whitening is applied)\n",
    "        aug_datagen.fit(input_data)\n",
    "\n",
    "        aug_generator = aug_datagen.flow(\n",
    "            input_data,\n",
    "            input_target,\n",
    "            batch_size = batch_size,\n",
    "            shuffle = True\n",
    "            #save_to_dir=os.path.join(file_path,'train_aug'),\n",
    "            #save_prefix='train_'\n",
    "            )\n",
    "\n",
    "    if type == \"validation\":\n",
    "        # this is the augmentation configuration we will use for validation:\n",
    "        # only rescaling\n",
    "        aug_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "        aug_generator = aug_datagen.flow(\n",
    "            input_data,\n",
    "            input_target,\n",
    "            batch_size = batch_size,\n",
    "            shuffle = True\n",
    "            )\n",
    "\n",
    "    if type == \"predict\":\n",
    "        # this is the augmentation configuration we will use for validation:\n",
    "        # only rescaling\n",
    "        aug_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "        aug_generator = aug_datagen.flow(\n",
    "            input_data,\n",
    "            None,\n",
    "            batch_size = batch_size,\n",
    "            shuffle = False\n",
    "            )\n",
    "\n",
    "    return aug_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation and model fitting for Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def Get_model(num_category):\n",
    "\n",
    "    if model_classifier == \"Inception\":\n",
    "        clf = CNN_Inceptionv03_Classifier(img_rows, img_cols, color_type_global,num_category)\n",
    "    elif model_classifier == \"Classifier1\":\n",
    "        clf = CNN_Classifier1(img_rows, img_cols, color_type_global,num_category)\n",
    "    elif model_classifier == \"VGG16\":\n",
    "        clf = CNN_VGG16_Classifier(img_rows, img_cols, color_type_global,num_category)\n",
    "    elif model_classifier == \"Inception_inbuilt\":\n",
    "        clf = CNN_Inceptionv03_inbuilt_Classifier(img_rows, img_cols, color_type_global,num_category)\n",
    "    elif model_classifier == \"VGG16_inbuilt\":\n",
    "        clf = CNN_VGG16_inbuilt_Classifier(img_rows, img_cols, color_type_global,num_category)\n",
    "    else:\n",
    "        clf = CNN_Classifier1(img_rows, img_cols, color_type_global,num_category)\n",
    "\n",
    "    return  clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation and model fitting for Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def Nfold_Training(X, y, target_vect):\n",
    "\n",
    "    print(\"Starting Model Training....... at Time: %s\" %(tm.strftime(\"%H:%M:%S\")))\n",
    "    start_time  = time.time()\n",
    "    random_state = 21\n",
    "\n",
    "    num_category = len(pd.DataFrame(y).columns)\n",
    "    print(\"Number of categories to predict: \"+ str(num_category))\n",
    "\n",
    "    yfull_train = dict()\n",
    "    yfull_test = []\n",
    "    num_fold = 0\n",
    "    sum_score = 0\n",
    "\n",
    "    X =np.array(X)\n",
    "    scores=[]\n",
    "\n",
    "    X, y = shuffle(X, y)\n",
    "    #ss = StratifiedShuffleSplit(target_vect, n_iter=nfolds, test_size=(1.0/nfolds),random_state=random_state)\n",
    "    ss = KFold(len(y), n_folds=nfolds,shuffle=True,random_state=random_state)\n",
    "\n",
    "    i = 1\n",
    "\n",
    "    for trainCV, testCV in ss:\n",
    "        X_train, X_test= X[trainCV], X[testCV]\n",
    "        Y_train, Y_test= y[trainCV], y[testCV]\n",
    "\n",
    "        X_train = X_train.astype('float32')\n",
    "        X_test  = X_test.astype('float32')\n",
    "\n",
    "        print('Split train: ', len(X_train), len(Y_train))\n",
    "        print('Split valid: ', len(X_test), len(Y_test))\n",
    "\n",
    "        clf = Get_model(num_category)\n",
    "\n",
    "        #kfold_weights_path = os.path.join(file_path, 'cache', 'weights_kfold_' + str(num_fold) + '.h5')\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', patience=20, verbose=0)\n",
    "           #,ModelCheckpoint(best_model_file, monitor='val_loss', save_best_only=True, verbose=0),\n",
    "        ]\n",
    "\n",
    "        if data_aug:\n",
    "            print(\"using fit_generator\")\n",
    "            train_generator = data_augmentation(X_train,Y_train,type=\"train\")\n",
    "            validation_generator = data_augmentation(X_test,Y_test,type=\"validation\")\n",
    "\n",
    "            clf.fit_generator(\n",
    "                    train_generator,\n",
    "                    samples_per_epoch = len(X_train)*10,\n",
    "                    nb_epoch = nb_epoch,\n",
    "                    validation_data = validation_generator,\n",
    "                    nb_val_samples = len(X_test),\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "        else:\n",
    "            clf.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                      shuffle=True, verbose=1, validation_data=(X_test, Y_test),\n",
    "                      callbacks=callbacks)\n",
    "\n",
    "        model_fn = os.path.join(file_path_orig,'models/') + 'model_iteration_' + str(i)+\".h5\"\n",
    "        clf.save_weights(model_fn)\n",
    "\n",
    "        #predict validation dataset\n",
    "        if data_aug:\n",
    "            Y_pred = clf.predict_generator(validation_generator, len(X_test))\n",
    "        else:\n",
    "            if predict_proba:\n",
    "                Y_pred=clf.predict_proba(X_test,batch_size=batch_size, verbose=1)\n",
    "            else:\n",
    "                Y_pred = clf.predict(X_test, batch_size=batch_size, verbose=1)\n",
    "\n",
    "        scores.append(log_loss(Y_test, Y_pred))\n",
    "\n",
    "        print(\" %d-iteration... %s \" % (i,scores))\n",
    "\n",
    "        i = i + 1\n",
    "\n",
    "    #Average ROC from cross validation\n",
    "    scores=np.array(scores)\n",
    "    print (\"Normal CV Score:\",np.mean(scores))\n",
    "\n",
    "    end_time  = time.time()\n",
    "    print(\"Ending Model Training....... at Time: %s\" %(tm.strftime(\"%H:%M:%S\")))\n",
    "    print(\"Training time taken for \"+str(nfolds)+\" models : \" + str(int(end_time - start_time)) +\" seconds\")\n",
    "\n",
    "    return yfull_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation and model fitting for Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Model_prediction(X, y,test_id):\n",
    "\n",
    "    print(\"Starting Model Classification....... at Time: %s\" %(tm.strftime(\"%H:%M:%S\")))\n",
    "    start_time  = time.time()\n",
    "\n",
    "    random_state = 42\n",
    "    num_category = len(pd.DataFrame(y).columns)\n",
    "\n",
    "    yfull_test = []\n",
    "    for model in glob.glob(os.path.join(file_path_orig,\"models\",\"*\")):\n",
    "        X_test =np.array(X)\n",
    "\n",
    "        clf = Get_model(num_category)\n",
    "\n",
    "        print(\"model \"+str(model))\n",
    "        clf.load_weights(model)\n",
    "\n",
    "        if data_aug:\n",
    "\n",
    "            validation_generator = data_augmentation(X_test,np.array(),type=\"predict\")\n",
    "            Y_pred = clf.predict_generator(validation_generator, len(X_test))\n",
    "        else:\n",
    "            if predict_proba:\n",
    "                Y_pred=clf.predict_proba(X_test,batch_size=batch_size, verbose=1)\n",
    "            else:\n",
    "                Y_pred = clf.predict(X_test, batch_size=batch_size, verbose=1)\n",
    "\n",
    "        yfull_test.append(Y_pred)\n",
    "\n",
    "    test_res = Merge_CV_folds_mean(yfull_test, nfolds)\n",
    "\n",
    "    Create_final_output_file(test_res, test_id)\n",
    "\n",
    "    print(\"***************Ending Kfold Cross validation***************\")\n",
    "\n",
    "    end_time  = time.time()\n",
    "    print(\"Classifcation time taken for \"+str(nfolds)+\" models : \" + str(int(end_time - start_time)) +\" seconds\")\n",
    "\n",
    "    return yfull_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model building and cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def Data_Munging(process_data, process_target, process_target_vect, process_id):\n",
    "    global input_shape, channel_axis\n",
    "\n",
    "    #if K.image_dim_ordering() == 'th':\n",
    "    if K.backend() == 'theano':\n",
    "        print(\"using Theano model\")\n",
    "        process_data = process_data.reshape(process_data.shape[0], color_type_global, img_rows, img_cols)\n",
    "        input_shape = (color_type_global, img_rows, img_cols)\n",
    "        channel_axis = 1\n",
    "    else:\n",
    "        print(\"using Tensorflow model\")\n",
    "        process_data = process_data.reshape(process_data.shape[0], img_rows, img_cols, color_type_global)\n",
    "        input_shape = (img_rows, img_cols, color_type_global)\n",
    "        channel_axis = 3\n",
    "        K.get_session().run(tf1.global_variables_initializer())\n",
    "\n",
    "    return process_data, process_target,process_target_vect, process_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read image using open cv and convert to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_im_cv2_mod(path, img_rows, img_cols, color_type):\n",
    "\n",
    "    # Load as grayscale\n",
    "    if color_type == 1:\n",
    "        img = cv2.imread(path, 0)\n",
    "    else:\n",
    "        img = cv2.imread(path)\n",
    "\n",
    "    resized = cv2.resize(img, (img_cols, img_rows), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    return resized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and Load train data from sub folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(img_rows, img_cols, color_type,sub_folder):\n",
    "    X_full = []\n",
    "    X_full_id = []\n",
    "    y_full = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    print('Read images')\n",
    "    for tf in sub_folder:\n",
    "        path = os.path.join(file_path, tf, '*.*')\n",
    "        files = glob.glob(path)\n",
    "        print('Load folder %s , Total files :- %d' %(format(tf),len(files)))\n",
    "\n",
    "        for fl in files:\n",
    "            flbase = os.path.basename(fl)\n",
    "            img = get_im_cv2_mod(fl, img_rows, img_cols, color_type)\n",
    "            X_full.append(img)\n",
    "            X_full_id.append(tf+\"_\"+flbase)\n",
    "            y_full.append(tf)\n",
    "\n",
    "    print('Read data time: {} seconds'.format(round(time.time() - start_time, 2)))\n",
    "    return X_full, y_full, X_full_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Read the Train images and convert to arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Get_image_data(img_rows, img_cols, color_type,sub_folder):\n",
    "\n",
    "    global lbl_y\n",
    "\n",
    "    process_data, process_target, process_id = load_data(img_rows, img_cols, color_type_global,sub_folder)\n",
    "\n",
    "    process_data = np.array(process_data, dtype=np.uint8)\n",
    "\n",
    "    lbl_y = preprocessing.LabelEncoder()\n",
    "    lbl_y.fit(list(process_target))\n",
    "    process_target = lbl_y.transform(process_target)\n",
    "    process_target_vect = process_target\n",
    "\n",
    "    process_target = np_utils.to_categorical(process_target)\n",
    "    process_target = np.array(process_target, dtype=np.uint8)\n",
    "\n",
    "    if color_type == 1:\n",
    "        process_data = process_data.reshape(process_data.shape[0], 1, img_rows, img_cols)\n",
    "    else:\n",
    "        process_data = process_data.transpose((0, 3, 1, 2))\n",
    "\n",
    "    process_data = process_data.astype('float32')\n",
    "    process_data /= 255\n",
    "    process_data -= 0.5\n",
    "    process_data *= 2.\n",
    "\n",
    "    print('Train shape:', process_data.shape)\n",
    "    print(process_data.shape[0], 'train samples')\n",
    "\n",
    "    return process_data, process_target, process_target_vect, process_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleansing , feature scaling , splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def Data_Loading():\n",
    "\n",
    "    print(\"Starting Data Loading....... at Time: %s\" % (tm.strftime(\"%H:%M:%S\")))\n",
    "    sub_folder = []\n",
    "    for folder in glob.glob(os.path.join(file_path,\"*/\")):\n",
    "        sub_folder.append(os.path.basename(os.path.dirname(folder)))\n",
    "\n",
    "    full_data, full_target, full_target_vect, full_id = Get_image_data(img_rows, img_cols, color_type_global,sub_folder)\n",
    "    print(\"Ending Data Loading....... at Time: %s\" % (tm.strftime(\"%H:%M:%S\")))\n",
    "\n",
    "    return full_data, full_target, full_target_vect, full_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main(process_type=\"train\", processor_type=\"gpu\"):\n",
    "\n",
    "    pd.set_option('display.width', 200)\n",
    "    pd.set_option('display.height', 500)\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    global file_path, use_cache, train_folder, test_folder, restore_from_last_checkpoint,\\\n",
    "        img_rows,img_cols,color_type_global,nb_epoch,batch_size,predict_proba,orig_input,data_aug,\\\n",
    "        load_data_flag,file_path_orig,nfolds,model_classifier\n",
    "\n",
    "    use_cache = 0\n",
    "    restore_from_last_checkpoint = 0\n",
    "\n",
    "    color_type_global = 3\n",
    "\n",
    "    predict_proba = True\n",
    "    train_folder = 'train'\n",
    "    test_folder = 'test'\n",
    "    data_aug = False\n",
    "    load_data_flag = False\n",
    "\n",
    "    #Key parms to set up\n",
    "    ####################################################################################################################\n",
    "    img_rows, img_cols = 64, 64\n",
    "    #img_rows, img_cols = 299, 299\n",
    "    batch_size = 64\n",
    "    nb_epoch = 100\n",
    "    nfolds=10\n",
    "    model_classifier  = \"Classifier1\"\n",
    "    ####################################################################################################################\n",
    "\n",
    "    # assumes files will in this directory\n",
    "    file_path_orig = os.path.abspath(\"att_faces\")\n",
    "    \n",
    "    #set up processor type (cpu or gpu)\n",
    "    if processor_type == 'cpu':\n",
    "        processor_type = '/cpu:0'\n",
    "    else:\n",
    "        processor_type = '/gpu:0'\n",
    "\n",
    "    if not(os.path.isdir(os.path.join(file_path_orig, \"models\"))):\n",
    "        os.makedirs(os.path.join(file_path_orig, \"models\"))\n",
    "\n",
    "    if not(os.path.isdir(os.path.join(file_path_orig, \"results\"))):\n",
    "        os.makedirs(os.path.join(file_path_orig, \"results\"))\n",
    "\n",
    "    if process_type.lower().strip() == 'train':\n",
    "        print(\"Training in progress....\")\n",
    "        file_path = os.path.join(file_path_orig, \"train\")\n",
    "    else:\n",
    "        print(\"Classification in progress....\")\n",
    "        file_path = os.path.join(file_path_orig, \"test\")\n",
    "\n",
    "    with tf1.device(processor_type):\n",
    "        full_data, full_target, full_target_vect, full_id = Data_Loading()\n",
    "        process_data, process_target, process_target_vect, process_id = Data_Munging(full_data, full_target, full_target_vect, full_id)\n",
    "\n",
    "        if process_type.lower().strip() == 'train':\n",
    "                yfull_test = Nfold_Training(process_data, process_target,process_target_vect)\n",
    "\n",
    "        else:\n",
    "            Model_prediction(process_data, process_target,process_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~76 seconds with gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time\n",
    "main(process_type=\"train\", processor_type=\"gpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~705 seconds with cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time\n",
    "main(process_type=\"train\", processor_type=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
